# -*- coding: utf-8 -*-
"""GP-BILSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13vcR5p2J_HqHgjfdWLBYmdvBDHtSm_ID
"""

!pip install hdf5storage
from google.colab import drive
drive.mount('/content/drive')
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import h5py
import numpy as np
filepath = '/content/drive/My Drive/GP-Decoding-Human-Brain-Activity/Dataset/data/erp_analyses/n24_SingleTrialEEG.mat'
import hdf5storage
mat = hdf5storage.loadmat(filepath)
dataS = mat["allsubj_eeg"]
data = np.zeros((192*24,128,1230), dtype= 'd')

print(data.shape)
print(dataS.shape)

for i in range(24):
    for f in range(192):
         data[i*192+f,:,:] = dataS[i,:,:,f]
         print(i)
dataset = data
with h5py.File('random.hdf5', 'w') as f:
    dset = f.create_dataset("default", data=dataset)

training = data[:3000,:,:]
testing=data[3000:,:,:]
print(training.shape)
print(testing.shape)

# start
!pip install hdf5storage
from google.colab import drive
drive.mount('/content/drive')
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import h5py
import numpy as np
import hdf5storage
filepath = '/content/drive/My Drive/GP-Decoding-Human-Brain-Activity/Dataset/random.hdf5'
f1 = h5py.File(filepath, 'r')
X1 = f1['default']
data= np.array(X1.value)
x_train = data[:3000,:,:]
testing = data[3000:,:,:]
print (x_train.shape)
print (testing.shape)

from os import listdir
import scipy.io
import math
from sklearn import preprocessing
import numpy as np
def find_filenames(path_to_dir):
    filenames = listdir(path_to_dir)
    return [filename for filename in filenames]

y_train = []
fileNames11 = find_filenames("/content/drive/My Drive/GP-Decoding-Human-Brain-Activity/Dataset/data/data_summary")
print(type(fileNames11))
label = np.empty(4608,dtype=object)
print(label)
f=0
for summary in fileNames11:
    mat = scipy.io.loadmat('/content/drive/My Drive/GP-Decoding-Human-Brain-Activity/Dataset/data/data_summary/'+ summary)
    temp = mat['experiment_table']
    for i in range(192):
         label[f*192+i] = temp[i][8]
        #  y_train.append(temp[i][8])
         if int(temp[i][4][0]) > 96:
           y_train.append(math.ceil((int(temp[i][4][0]) - 96)/3))
         elif int(temp[i][4][0]) == 1 or int(temp[i][4][0]) == 2:
           y_train.append(1)
         else:
           y_train.append(math.ceil((int(temp[i][4][0]))/3))
    f=f+1
print(label[2000])
print(label)
# y_train = np.concatenate(y_train,axis=0)
print(y_train)

testing_data = np.array(y_train)
y_train = testing_data[:3000]
y_testing = testing_data[3000:]
print(x_train)
print(y_train)
print(x_train.shape)
print(y_train.shape)
normalized_x_train = preprocessing.normalize([x_train])
normalized_y_train = preprocessing.normalize([y_train])
normalized_x_test = preprocessing.normalize([testing])
normalized_y_test = preprocessing.normalize([y_testing])
print(normalized_x_train)
print(normalized_x_test)
print(normalized_x_train.shape)
print(normalized_x_test.shape)

import math
import pandas as pd
from random import random
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import TimeDistributed
from keras.layers import Bidirectional
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt


# # define LSTM
# model = Sequential()
# model.add(Bidirectional(LSTM(16, return_sequences=True), input_shape=(x_train.shape[1], x_train.shape[2])))
# model.add(TimeDistributed(Dense(y_train.shape[1], activation='softmax')))
# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# # Time series define LSTM
# model = Sequential()
# model.add(LSTM(200, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))
# model.add(Dense(100, activation='relu'))
# model.add(Dense(y_train.shape[0]))
# model.compile(loss='mse', optimizer='adam')

#double layers
model = Sequential()
model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(128, 1230)))
model.add(Bidirectional(LSTM(128)))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='softmax'))
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# # With custom backward layer
# model = Sequential()
# forward_layer = LSTM(64, return_sequences=True)
# backward_layer = LSTM(64, activation='softmax', return_sequences=True, go_backwards=True)
# model.add(Bidirectional(forward_layer, backward_layer=backward_layer, input_shape=(128, 1230)))
# model.add(Dense(32, activation='softmax'))
# model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

model.summary()
# model.fit(x_train, y_train, epochs=2, verbose=1)
model.fit(x_train, y_train, validation_data=(testing, y_testing), epochs=2, batch_size=128, verbose=1)

print("Evaluate on test data")
results = model.evaluate(testing, y_testing, batch_size=128, verbose=1)
print("test loss, test acc:", results)
print("Generate predictions for 9 samples")
predictions = model.predict(testing[:3])
print("predictions shape:", predictions.shape)
print("predictions:", predictions)

import math
import numpy as np
import pandas as pd
from random import random
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import TimeDistributed
from keras.layers import Bidirectional
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
features = (np.random.randint(10, size=(100, 1)))
print(features.shape)
training_dataset_length = math.ceil(len(features) * .75)
print(training_dataset_length)
scaler = MinMaxScaler(feature_range=(0, 1)) 
scaled_data = scaler.fit_transform(features)
train_data = scaled_data[0:training_dataset_length  , : ]

#Splitting the data
x_train=[]
y_train = []

for i in range(10, len(train_data)):
    x_train.append(train_data[i-10:i,0])
    y_train.append(train_data[i,0])

#Convert to numpy arrays
print("x_train befor respahe to numpy")
# print(x_train.shape)
print("y_train befor respahe to numpy")
# print(y_train.shape)
x_train, y_train = np.array(x_train), np.array(y_train)
print("x_train after respahe to numpy")
print(x_train.shape)
print("y_train aftr respahe to numpy")
print(y_train)

#Reshape the data into 3-D array
x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))
print("x_train aftr respahe to 3-D array")
print(x_train.shape)

from keras.layers import Dropout

# Initialising the RNN
model = Sequential()

model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))
model.add(Dropout(0.2))

# Adding a second LSTM layer and Dropout layer
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))

# Adding a third LSTM layer and Dropout layer
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))

# Adding a fourth LSTM layer and and Dropout layer
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

# Adding the output layer
# For Full connection layer we use dense
# As the output is 1D so we use unit=1
model.add(Dense(units = 1))

#compile and fit the model on 30 epochs
model.compile(optimizer = 'adam', loss = 'mean_squared_error')
model.fit(x_train, y_train, epochs = 30, batch_size = 50)

#Test data set
test_data = scaled_data[training_dataset_length - 10: , : ]

#splitting the x_test and y_test data sets
x_test = []
y_test =  features[training_dataset_length : , : ] 

for i in range(10,len(test_data)):
    x_test.append(test_data[i-10:i,0])
    
#Convert x_test to a numpy array 
x_test = np.array(x_test)

#Reshape the data into 3-D array
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))

#check predicted values
predictions = model.predict(x_test) 
#Undo scaling
predictions = scaler.inverse_transform(predictions)

#Calculate RMSE score
rmse=np.sqrt(np.mean(((predictions- y_test)**2)))
rmse