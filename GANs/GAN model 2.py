# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F5BhG7JpyGPzuCjS0qLyiwfmOjxaI4Oq
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

from google.colab import files
src = list(files.upload().values())[0]
open('model.py','wb').write(src)
import model

import numpy as np
import matplotlib.pyplot as plt
from time import time
import keras

from keras.models import Model, Sequential
from keras.optimizers import Adam
import keras.backend as K
from keras.utils.generic_utils import Progbar

from model import *

from PIL import Image
import os

def imagenet_data():
  imagenet_img = np.load('/content/drive/MyDrive/projects/imageNet_img.npy')
  imagenet_l = np.load('/content/drive/MyDrive/projects/imageNet_l.npy') -1
  cats = []
  i = 0
  for label in imagenet_l:
    if label == 20 :
      cats.append(imagenet_img[i])

    i =i + 1
  cats = np.asarray(cats)
  X = cats.astype('float32')
  X = X/255*2-1
  del cats , imagenet_l , imagenet_img 
  return X

def images_features():
  labels =np.load('/content/drive/MyDrive/projects/images_labels.npy')
  feature =[]
  image = []
  features = np.load('/content/drive/MyDrive/projects/EEG_features.npy')
  images = np.load('/content/drive/MyDrive/images.npy')
  for i in range(1996):
    if labels[i] == 20 :
        feature.append(features[i])
        image.append(images[i])
  feature = np.asarray(feature)
  image = np.asarray(image)
  X = image.astype('float32')
  X = X*2-1
  del  features ,images ,image
  return X , (feature //5)

X = imagenet_data()
Y, features = images_features()
X.shape

"""# New Section"""

#Hyperperemeter
BATCHSIZE=16
LEARNING_RATE = 0.0002
TRAINING_RATIO = 1
BETA_1 = 0.0
BETA_2 = 0.9
EPOCHS = 250
BN_MIMENTUM = 0.9
BN_EPSILON  = 0.00002
# Size vector to generate images from
SEED_SIZE = 228
SAVE_DIR = '/content/drive/MyDrive/GAN/attention/output/'

GENERATE_ROW_NUM = 8
GENERATE_BATCHSIZE = GENERATE_ROW_NUM*GENERATE_ROW_NUM

def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true*y_pred)

generator = BuildGenerator(bn_momentum=BN_MIMENTUM, bn_epsilon=BN_EPSILON)
discriminator = BuildDiscriminator()

Noise_input_for_training_generator = Input(shape=(SEED_SIZE,))
Generated_image                    = generator(Noise_input_for_training_generator)
Discriminator_output               = discriminator(Generated_image)
model_for_training_generator       = Model(Noise_input_for_training_generator, Discriminator_output)
print("model_for_training_generator")
discriminator.trainable = False
model_for_training_generator.summary()
model_for_training_generator.compile(optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), loss=wasserstein_loss)

Real_image                             = Input(shape=(64,64,3))
Noise_input_for_training_discriminator = Input(shape=(SEED_SIZE,))
Fake_image                             = generator(Noise_input_for_training_discriminator)
Discriminator_output_for_real          = discriminator(Real_image)
Discriminator_output_for_fake          = discriminator(Fake_image)

model_for_training_discriminator       = Model([Real_image,
                                                Noise_input_for_training_discriminator],
                                               [Discriminator_output_for_real,
                                                Discriminator_output_for_fake])
print("model_for_training_discriminator")
generator.trainable = False
discriminator.trainable = True
model_for_training_discriminator.compile(optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), loss=[wasserstein_loss, wasserstein_loss])
model_for_training_discriminator.summary()

real_y = np.ones((BATCHSIZE, 1), dtype=np.float32)
fake_y = -real_y

# Preview image 
PREVIEW_ROWS = 4
PREVIEW_COLS = 4

GENERATE_SQUARE = 64
DATA_PATH = '/content/drive/My Drive/GAN/attention'

def save_images(cnt,generated_images):

  image_array = np.full( (64*4,64*4, 3),255,dtype=np.uint8)

  #generated_images = 0.5 * generated_images + 0.5

  image_count = 0
  for row in range(PREVIEW_ROWS):
      for col in range(PREVIEW_COLS):
        r = row * (GENERATE_SQUARE) 
        c = col * (GENERATE_SQUARE) 
        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \
            = generated_images[image_count] * 255
        image_count += 1

          
  output_path = os.path.join(DATA_PATH,'output')
  if not os.path.exists(output_path):
    os.makedirs(output_path)
  
  filename = os.path.join(output_path,f"train-{cnt}.png")
  im = Image.fromarray(image_array)
  im.save(filename)

def save_array(cnt,images):
  output_path = '/content/drive/MyDrive/images'

  if not os.path.exists(output_path):
    os.makedirs(output_path)
  output_path = os.path.join('/content/drive/MyDrive/images',f'images-{cnt}.npy')
  np.save(output_path , images)

zeros = np.zeros([BATCHSIZE,128])
W_loss = []
discriminator_loss = []
generator_loss = []
for epoch in range(EPOCHS):
   # np.random.shuffle(X)
    if epoch > 150 :
      X = Y
    print("epoch {} of {}".format(epoch+1, EPOCHS))
    num_batches = int(X.shape[0] // BATCHSIZE)
    
    print("number of batches: {}".format(int(X.shape[0] // (BATCHSIZE))))
    
    progress_bar = Progbar(target=int(X.shape[0] // (BATCHSIZE)))
    
    start_time = time()
    for index in range(int(X.shape[0] // (BATCHSIZE))):
        progress_bar.update(index)
        discriminator_minibatches = X[index * BATCHSIZE:(index + 1) * BATCHSIZE]
        if epoch > 150 :
          fea = features[index * BATCHSIZE:(index + 1) * BATCHSIZE]

        for j in range(TRAINING_RATIO):
            image_batch = discriminator_minibatches[j * BATCHSIZE : (j + 1) * BATCHSIZE]
            if epoch > 150 :
              feat = fea[j * BATCHSIZE : (j + 1) * BATCHSIZE]
              noise = np.concatenate([np.random.randn(BATCHSIZE, 100),feat],1).astype(np.float32)
            else :
              noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
              

            discriminator.trainable = True
            generator.trainable = False
            discriminator_loss.append(model_for_training_discriminator.train_on_batch([image_batch, noise],
                                                                                      [real_y, fake_y]))
        if epoch > 150 :
          noise = np.concatenate([np.random.randn(BATCHSIZE, 100),feat],1).astype(np.float32)
        else :
          noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
        
        discriminator.trainable = False
        generator.trainable = True
        generator_loss.append(model_for_training_generator.train_on_batch(noise, real_y))
    
    print('\nepoch time: {}'.format(time()-start_time))
    
    if epoch > 150 :
      test_noise = np.concatenate([np.random.randn(BATCHSIZE, 100),features[:BATCHSIZE]],1).astype(np.float32)
    else:
      zeros = np.zeros([BATCHSIZE,128])
      test_noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
    

    W_real = model_for_training_generator.evaluate(test_noise, real_y)
    print(W_real)
    W_fake = model_for_training_generator.evaluate(test_noise, fake_y)
    print(W_fake)
    W_l = W_real+W_fake
    print('wasserstein_loss: {}'.format(W_l))
    W_loss.append(W_l)
    #Generate image
    generated_image = generator.predict(test_noise)
    generated_image = (generated_image+1)/2

    save_images(epoch,generated_image)

    if epoch % 50 == 0 :
      if epoch > 150 :
         generated_image = generator.predict(np.concatenate([np.random.randn(50, 100),features[:50]],1))
         generated_image = (generated_image+1)/2
         save_array(epoch,generated_image)
      else :
         generated_image = generator.predict(np.concatenate([np.random.randn(50, 100),np.zeros([50,128])],1))
         generated_image = (generated_image+1)/2
         save_array(epoch,generated_image)
