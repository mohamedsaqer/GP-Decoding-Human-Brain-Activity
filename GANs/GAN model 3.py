# -*- coding: utf-8 -*-
"""DC_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bwc0JNrR6Vp5cILF_mgTb8drELWLrNnq
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
src = list(files.upload().values())[0]
open('SpectralNormalizationKeras.py','wb').write(src)
import SpectralNormalizationKeras

import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Input, Dense, Conv2D, Add, Dot, Conv2DTranspose, Activation, Reshape, LeakyReLU, Flatten, BatchNormalization
from SpectralNormalizationKeras import DenseSN, ConvSN2D
from keras.models import Model, Sequential
from keras.optimizers import Adam
import keras.backend as K
from keras.utils.generic_utils import Progbar
from time import time
import os
from PIL import Image

def imagenet_data():
  imagenet_img = np.load('/content/drive/MyDrive/projects/imageNet_img.npy')
  imagenet_l = np.load('/content/drive/MyDrive/projects/imageNet_l.npy')-1
  cats = []
  i = 0
  for label in imagenet_l:
    if label == 9 :
      cats.append(imagenet_img[i])

    i =i + 1
  cats = np.asarray(cats)
  X = cats.astype('float32')
  X = X/255*2-1 
  del cats , imagenet_l , imagenet_img 
  return X

def images_features():
  labels =np.load('/content/drive/MyDrive/projects/images_labels.npy')
  feature =[]
  image = []
  features = np.load('/content/drive/MyDrive/projects/EEG_features.npy')
  images = np.load('/content/drive/MyDrive/images.npy')
  for i in range(1996):
    if labels[i] == 9 :
        feature.append(features[i])
        image.append(images[i])
  feature = np.asarray(feature)
  image = np.asarray(image)
  X = image.astype('float32')
  X = X*2-1
  del  features ,images ,image
  return X , (feature //5)

X = imagenet_data()
Y , features = images_features()


X.shape

#Hyperperemeter
BATCHSIZE=16
LEARNING_RATE = 0.0002
TRAINING_RATIO = 1
BETA_1 = 0.0
BETA_2 = 0.9
EPOCHS = 200
BN_MIMENTUM = 0.9
BN_EPSILON  = 0.00002
# Size vector to generate images from
SEED_SIZE = 228
SAVE_DIR = '/content/drive/MyDrive/GAN/attention/output/'

GENERATE_ROW_NUM = 8
GENERATE_BATCHSIZE = GENERATE_ROW_NUM*GENERATE_ROW_NUM

def BuildGenerator(summary=True):
    model = Sequential()
    model.add(Dense(4*4*512, kernel_initializer='glorot_uniform' , input_dim=SEED_SIZE))
    model.add(Reshape((4,4,512)))

    model.add(Conv2DTranspose(512, kernel_size=4, strides=2, padding='same', activation='relu',kernel_initializer='glorot_uniform'))
    model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MIMENTUM))

    model.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', activation='relu',kernel_initializer='glorot_uniform'))
    model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MIMENTUM))

    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu',kernel_initializer='glorot_uniform'))
    model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MIMENTUM))

    model.add(Conv2DTranspose(64,  kernel_size=4, strides=2, padding='same', activation='relu',kernel_initializer='glorot_uniform'))
    model.add(BatchNormalization(epsilon=BN_EPSILON, momentum=BN_MIMENTUM))

    model.add(Conv2DTranspose(3,   kernel_size=3, strides=1, padding='same', activation='tanh'))
    if summary:
        print("Generator")
        model.summary()
    return model

def BuildDiscriminator(summary=True):
    model = Sequential()
    model.add(ConvSN2D(64, kernel_size=3, strides=1,kernel_initializer='glorot_uniform', padding='same', input_shape=(64,64,3) ))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(64, kernel_size=4, strides=2,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(128, kernel_size=3, strides=1,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(128, kernel_size=4, strides=2,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(256, kernel_size=3, strides=1,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(256, kernel_size=4, strides=2,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))

    model.add(ConvSN2D(512, kernel_size=3, strides=1,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(ConvSN2D(512, kernel_size=4, strides=2,kernel_initializer='glorot_uniform', padding='same'))
    model.add(LeakyReLU(0.1))
    model.add(Flatten())
    model.add(DenseSN(1,kernel_initializer='glorot_uniform'))
    if summary:
        print('Discriminator')
        model.summary()
    return model

def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true*y_pred)

generator = BuildGenerator()
discriminator = BuildDiscriminator()

Noise_input_for_training_generator = Input(shape=(SEED_SIZE,))
Generated_image                    = generator(Noise_input_for_training_generator)
Discriminator_output               = discriminator(Generated_image)
model_for_training_generator       = Model(Noise_input_for_training_generator, Discriminator_output)
discriminator.trainable = False

model_for_training_generator.compile(optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), loss=wasserstein_loss)
print("model_for_training_generator")
model_for_training_generator.summary()

Real_image                             = Input(shape=(64,64,3))
Noise_input_for_training_discriminator = Input(shape=(SEED_SIZE,))
Fake_image                             = generator(Noise_input_for_training_discriminator)
Discriminator_output_for_real          = discriminator(Real_image)
Discriminator_output_for_fake          = discriminator(Fake_image)

model_for_training_discriminator       = Model([Real_image,
                                                Noise_input_for_training_discriminator],
                                               [Discriminator_output_for_real,
                                                Discriminator_output_for_fake])
print("model_for_training_discriminator")
generator.trainable = False
discriminator.trainable = True
model_for_training_discriminator.compile(optimizer=Adam(LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2), loss=[wasserstein_loss, wasserstein_loss])
model_for_training_discriminator.summary()

real_y = np.ones((BATCHSIZE, 1), dtype=np.float32)
fake_y = -real_y

# Preview image 
PREVIEW_ROWS = 4
PREVIEW_COLS = 4

GENERATE_SQUARE = 64
DATA_PATH = '/content/drive/My Drive/GAN/attention'

def save_images(cnt,generated_images):

  image_array = np.full( (64*4,64*4, 3),255,dtype=np.uint8)

  #generated_images = 0.5 * generated_images + 0.5

  image_count = 0
  for row in range(PREVIEW_ROWS):
      for col in range(PREVIEW_COLS):
        r = row * (GENERATE_SQUARE) 
        c = col * (GENERATE_SQUARE) 
        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \
            = generated_images[image_count] * 255
        image_count += 1

          
  output_path = os.path.join(DATA_PATH,'output')
  if not os.path.exists(output_path):
    os.makedirs(output_path)
  
  filename = os.path.join(output_path,f"train-{cnt}.png")
  im = Image.fromarray(image_array)
  im.save(filename)

zeros = np.zeros([BATCHSIZE,128])
W_loss = []
discriminator_loss = []
generator_loss = []
for epoch in range(EPOCHS):
   # np.random.shuffle(X)
    if epoch > 150 :
      X = Y
    print("epoch {} of {}".format(epoch+1, EPOCHS))
    num_batches = int(X.shape[0] // BATCHSIZE)
    
    print("number of batches: {}".format(int(X.shape[0] // (BATCHSIZE))))
    
    progress_bar = Progbar(target=int(X.shape[0] // (BATCHSIZE)))
    
    start_time = time()
    for index in range(int(X.shape[0] // (BATCHSIZE))):
        progress_bar.update(index)
        discriminator_minibatches = X[index * BATCHSIZE:(index + 1) * BATCHSIZE]
        if epoch > 150 :
          fea = features[index * BATCHSIZE:(index + 1) * BATCHSIZE]

        for j in range(TRAINING_RATIO):
            image_batch = discriminator_minibatches[j * BATCHSIZE : (j + 1) * BATCHSIZE]
            if epoch > 150 :
              feat = fea[j * BATCHSIZE : (j + 1) * BATCHSIZE]
              noise = np.concatenate([np.random.randn(BATCHSIZE, 100),feat],1).astype(np.float32)
            else :
              noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
              

            discriminator.trainable = True
            generator.trainable = False
            discriminator_loss.append(model_for_training_discriminator.train_on_batch([image_batch, noise],
                                                                                      [real_y, fake_y]))
        if epoch > 150 :
          noise = np.concatenate([np.random.randn(BATCHSIZE, 100),feat],1).astype(np.float32)
        else :
          noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
        
        discriminator.trainable = False
        generator.trainable = True
        generator_loss.append(model_for_training_generator.train_on_batch(noise, real_y))
    
    print('\nepoch time: {}'.format(time()-start_time))
    
    if epoch > 150 :
      test_noise = np.concatenate([np.random.randn(BATCHSIZE, 100),features[:BATCHSIZE]],1).astype(np.float32)
    else:
      zeros = np.zeros([BATCHSIZE,128])
      test_noise = np.concatenate([np.random.randn(BATCHSIZE, 100),zeros],1).astype(np.float32)
    

    W_real = model_for_training_generator.evaluate(test_noise, real_y)
    print(W_real)
    W_fake = model_for_training_generator.evaluate(test_noise, fake_y)
    print(W_fake)
    W_l = W_real+W_fake
    print('wasserstein_loss: {}'.format(W_l))
    W_loss.append(W_l)
    #Generate image
    generated_image = generator.predict(test_noise)
    generated_image = (generated_image+1)/2

    save_images(epoch,generated_image)

f, a = plt.subplots(2, 10, figsize=(40, 10))
for i in range(10):
    a[0][i].imshow(X[i]+.5)
    a[1][i].imshow(generated_image[i])