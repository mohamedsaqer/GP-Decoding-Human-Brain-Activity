# -*- coding: utf-8 -*-
"""Temp transformer40 sub data set.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TJIsnZRfUNwgZLYSw31wmmwO9ynJhGyR
"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/My Drive/Dataset/eeg/'

import tensorflow as tf
from tensorflow import keras  
from tensorflow.keras import layers
import numpy as np
#import h5py
#import csv
import torch

model=torch.load(path+'eeg_signals_128_sequential_band_all_with_mean_std.pth')

keys = list(model.keys())
keys

im = model['images']
print(len(im),im)

la = model['labels']
print(len(la),la,sep='\n')

me = model['means']
print(len(me),list(me),sep='\n')

st = model['stddevs']
print(len(st),list(st),sep='\n')

d=np.array(model['dataset'])
print(type(d),d.shape,d[1].keys(),sep='\n')
# print(x_train[0,:,0])
d1 = np.array(d[1])
print(d[1]['eeg'],len(d[1]['eeg']))
print(d[10000]['image'])
print(d[1]['label'])
print(d[1]['subject'])



train_size = 9572
test_size = 2393
l_sh = 50
r_sh = 250
diff = r_sh - l_sh

eeg = []
label = []
for i in range(len(d)):
  h = tf.make_ndarray(tf.make_tensor_proto(d[i]['eeg']))
  eeg.append(h)
  label.append((d[i]['label']))
label = np.array(label)

temp = np.zeros(shape=(len(d),128,diff))
for i in range(len(d)):
  for j in range(128):
     temp[i][j] = list(eeg[i][j][l_sh:r_sh])
eeg = temp
del(temp)

x_train = eeg[:train_size,:,:]
y_train = label[:train_size]
x_test = eeg[train_size:,:,:]
y_test = label[train_size:]

print(type(x_train),x_train.shape)
print(type(y_train),y_train.shape)

print(type(x_test),x_test.shape)
print(type(y_test),y_test.shape)

del(d)
del(model)
del(eeg)
del(label)

# print(x_train[0],x_test[0],x_train.dtype,sep='\n')
print(x_train[0,:,0])

maxc = 128
maxt = 200
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(maxc, maxt)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=10)

import keras
from keras.layers import Input, Dense, LSTM
from keras.models import Model

from keras.utils import to_categorical
y_train = to_categorical(y_train, num_classes=40)
y_test = to_categorical(y_test, num_classes=40)

# defining the keras callbacks to be used while training the network

from keras.callbacks import ModelCheckpoint, EarlyStopping
modelcheckpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', verbose=1, patience=10)

def lstm_model(output_dim=300, num_classes=40):

    input_layer  = Input(shape=(128, 200))
    lstm1 = LSTM(output_dim, return_sequences=False)(input_layer)
    dense1 = Dense(128, activation='tanh')(lstm1)  
    output = Dense(num_classes, activation='softmax')(dense1)
    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

#one-hot-encoding the labels of classes


# creating the model object and putting the model to training and testing  
# we use validation data as test data and vice versa because we are not performing any kind of model selection 

model = lstm_model(output_dim=300, num_classes=40)
print('MODEL TRAINING BEGINNING------------->')
model.fit(x_train, y_train, batch_size=16, epochs=2000, callbacks=[modelcheckpoint, earlystopping], validation_data=(x_test, y_test))
print('EVALUATING MODEL ON TEST SET------------>')
print(model.evaluate(x_test, y_test))