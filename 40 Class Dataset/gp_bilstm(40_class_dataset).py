# -*- coding: utf-8 -*-
"""GP-BILSTM(40 class dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r__g7jqXFEuWBu3Z71xHYhLjvb_UpSJy
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import torch
from tensorflow import keras
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from tensorflow.keras import Input , Model
from tensorflow.keras.layers import Flatten, Dense, Bidirectional, TimeDistributed, LSTM
from torchvision import transforms
import torch.utils.data as data
import torch.nn
import torch.optim as optim
from torch.autograd import Variable
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.keras import backend as K
from keras.models import Sequential
import matplotlib.pyplot as plt
from sklearn import preprocessing

class eegloader(data.Dataset):
	def __init__(self, data_path, split_path, dtype='train', data_dir='./', split_no=0, dlen=160, stpt=320, nch=128):

		data = torch.load(data_dir + data_path)
		split = torch.load(data_dir + split_path)

		self.mean = data['means']
		self.stdev = data['stddevs']
		self.labels = split['splits'][split_no][dtype]

		self.data = []
		for l in self.labels:
			self.data.append(data['dataset'][l])

		assert len(self.data)==len(self.labels)
		self.dlen = dlen
		self.stpt = stpt
		self.nch = nch

	def __len__(self):
		return len(self.data)
	
	def __getitem__(self, idx):
		nch  = self.nch
		dlen = self.dlen 
		stpt = self.stpt
	
		x = np.zeros((nch,dlen))
		y = self.data[idx]['label']
		s = self.data[idx]['subject'] 
		
		x = torch.from_numpy(x)
		x[:,:min(int(self.data[idx]['eeg'].shape[1]),dlen)] = self.data[idx]['eeg'][:,stpt:stpt+dlen]
		x = x.type(torch.FloatTensor).sub(self.mean.expand(nch,dlen))/ self.stdev.expand(nch,dlen)

		return x, y, s

data_dir = '/content/drive/MyDrive/GP/'
batch_size = 128
data_path = 'eeg_signals_128_sequential_band_all_with_mean_std.pth'
split_path = 'block_splits_by_image.pth'
split_no = 3

#trainset = data.DataLoader(eegloader(data_path, split_path, dtype='train', data_dir=data_dir, split_no=split_no),batch_size=batch_size, shuffle=True)
#valset = data.DataLoader(eegloader(data_path, split_path, dtype='val', data_dir=data_dir, split_no=split_no),batch_size=batch_size)
#testset = data.DataLoader(eegloader(data_path, split_path, dtype='test', data_dir=data_dir, split_no=split_no),batch_size=batch_size)
#print('data loaded')

x = eegloader(data_path, split_path, dtype='train', data_dir=data_dir, split_no=split_no)
y = eegloader(data_path, split_path, dtype='val', data_dir=data_dir, split_no=split_no)
z = eegloader(data_path, split_path, dtype='test', data_dir=data_dir, split_no=split_no)

print(x[0][0])

training_x=[]
training_y=[]
testing_x=[]
testing_y=[]
for i in range(7972):
  training_x.append(tf.make_ndarray(tf.make_tensor_proto(x[i][0])))
  training_y.append(x[i][1])

for i in range(1997):
  testing_x.append(tf.make_ndarray(tf.make_tensor_proto(z[i][0])))
  testing_y.append(z[i][1])

training = np.array(training_x)
labeling = np.array(training_y)
test_x = np.array(testing_x)
test_y = np.array(testing_y)

vall_x = []
vall_y = []

for i in range(1996):
  vall_x.append(tf.make_ndarray(tf.make_tensor_proto(y[i][0])))
  vall_y.append(y[i][1])

val_x = np.array(vall_x)
val_y = np.array(vall_y)

labels = []
for i in range(7972):
  labels.append(tf.keras.utils.to_categorical(training_y[i], num_classes=40))

y_train = to_categorical(labeling, num_classes=40)

y_test = to_categorical(test_y, num_classes=40)

# defining the keras callbacks to be used while training the network
modelcheckpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', verbose=1, patience=10)

class Attention(Layer):
    
    def __init__(self, return_sequences=True):
        self.return_sequences = return_sequences
        super(Attention,self).__init__()
        
    def build(self, input_shape):
        
        self.W=self.add_weight(name="att_weight", shape=(input_shape[-1],1),
                               initializer="normal")
        self.b=self.add_weight(name="att_bias", shape=(input_shape[1],1),
                               initializer="zeros")
        
        super(Attention,self).build(input_shape)
        
    def call(self, x):
        
        e = K.tanh(K.dot(x,self.W)+self.b)
        a = K.softmax(e, axis=1)
        output = x*a
        
        if self.return_sequences:
            return output
        
        return K.sum(output, axis=1)

model = Sequential()
model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(128, 160)))
model.add(Attention(return_sequences=True)) # receive 3D and output 3D
# model.add(Bidirectional(LSTM(300)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(40, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# defining the keras callbacks to be used while training the network
from keras.callbacks import ModelCheckpoint, EarlyStopping
modelcheckpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', verbose=1, patience=10)

model.summary()

print('MODEL TRAINING BEGINNING------------->')
model.fit(training, y_train, batch_size=16, epochs=100, callbacks=[modelcheckpoint, earlystopping], validation_data=(test_x, y_test))

print('EVALUATING MODEL ON TEST SET------------>')
print(model.evaluate(test_x, y_test))

# model predict
x1 = model.predict(val_x)
print(np.argmax(x1[1]),
      x1.shape,
      val_y[0:10], 
      sep='\n')

t = 0
f = 0
for i in range(1996):
  tst = np.argmax(x1[i])
  if(tst==val_y[i]):t+=1
  else:f+=1

print(t,f,t/1997)