# -*- coding: utf-8 -*-
"""trasformer_EEG_40_class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aU-V7zDTtgTIEB0MLcz7FPaZgozGoJXp
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/Dataset/eeg/data_import.py /content
from data_import import data_load 

strt= 50 #start
flen = 250  #length
x_train,y_train,x_test,y_test,x_val,y_val = data_load(strt ,flen)

from tensorflow.python.keras.backend import clear_session
import tensorflow as tf,keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pylab as plt
import torch.utils.data as data
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
import torch
import keras
from tensorflow.keras.models import Sequential
from keras.layers import Input, Dense, LSTM,Dropout
from tensorflow.keras.utils import to_categorical
from keras.models import Model
from torchvision import transforms

y_train = to_categorical(y_train, num_classes=40)
y_val = to_categorical(y_val, num_classes=40)
y_test = to_categorical(y_test, num_classes=40)

"""#Transformer

"""

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        
    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)        
        return self.layernorm2(out1 + ffn_output)


class myCallback(tf.keras.callbacks.Callback):
	def on_epoch_end(self, epoch, logs={}):
		if(logs.get('val_accuracy') >= 0.995):
			print("\nReached %2.2f%% accuracy, so stopping training!!" %(0.995))
			self.model.stop_training = True

num_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer

sequence_input = tf.keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))
transformer_block = TransformerBlock(x_train.shape[2], num_heads, ff_dim)
x = transformer_block(sequence_input)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dense(128, activation="relu")(x)
outputs = layers.Dense(40, activation="softmax")(x)
model = tf.keras.Model(inputs=sequence_input, outputs=outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

history = model.fit(x_train, y_train, validation_data=(x_test, y_test),
                   epochs=45, batch_size=32,callbacks = myCallback())
print(model.evaluate(x_test, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['loss'])
plt.plot(history.history['val_accuracy'])
plt.plot(history.history['val_loss'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['accuarcy', 'loss','val_accuracy','val_loss'], loc='upper right')
plt.show()

# extract = Model(model.inputs, model.layers[-2].output)
# features = extract.predict(x_train)
# extract.summary()