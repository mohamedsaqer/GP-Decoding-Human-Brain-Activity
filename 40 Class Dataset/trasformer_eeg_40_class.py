# -*- coding: utf-8 -*-
"""trasformer EEG 40 class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qlvDmWJwEOuBw2_XQie2o3w_LhFFRJGe
"""

from google.colab import drive
drive.mount('/content/drive')
from tensorflow.python.keras.backend import clear_session
import tensorflow as tf,keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pylab as plt
import torch.utils.data as data
#import h5py
#import csv
from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint
import torch
import keras
from tensorflow.keras.models import Sequential
from keras.layers import Input, Dense, LSTM,Dropout
from keras.utils import to_categorical
from keras.models import Model
from torchvision import transforms

"""# class EEGloader
loadData

"""

class eegloader(data.Dataset):
	def __init__(self, data_path, labels, data_dir='./', dlen=160, stpt=320, nch=128):

		data = torch.load(data_dir + data_path)
		self.mean = data['means']
		self.stdev = data['stddevs']
		self.labels = labels
		self.data = []
    
		for l in self.labels:
			self.data.append(data['dataset'][l])

		assert len(self.data)==len(self.labels)
		self.dlen = dlen
		self.stpt = stpt
		self.nch = nch

	def __len__(self):
		return len(self.data)
	
	def __getitem__(self, idx):
		nch  = self.nch
		dlen = self.dlen 
		stpt = self.stpt
	
		x = np.zeros((nch,dlen))
		y = self.data[idx]['label']
		s = self.data[idx]['subject'] 
		
		x = torch.from_numpy(x)
		x[:,:min(int(self.data[idx]['eeg'].shape[1]),dlen)] = self.data[idx]['eeg'][:,stpt:stpt+dlen]
		x = x.type(torch.FloatTensor).sub(self.mean.expand(nch,dlen))/ self.stdev.expand(nch,dlen)

		return x,y,s

def load_data():
  
  data_dir = path
  

  x = eegloader(data_path, labels, data_dir=data_dir,dlen=l_sh, stpt=r_sh)
  x_train=[]
  y_train=[]
  x_test=[]
  y_test=[]
  x_val =[]
  y_val =[]
  train_size = int(len(x)*0.8)
  val_size = int(len(x)*0.1)
  test_size = len(x) - train_size - val_size

  for i in range(train_size):
    x_train.append(tf.make_ndarray(tf.make_tensor_proto(x[i][0])))
    y_train.append(x[i][1])

  for i in range(val_size):
    x_val.append(tf.make_ndarray(tf.make_tensor_proto(x[i][0])))
    y_val.append(x[i][1])

  for i in range(test_size):
    x_test.append(tf.make_ndarray(tf.make_tensor_proto(x[i][0])))
    y_test.append(x[i][1])


  x_train = np.array(x_train)
  y_train = np.array(y_train)
  x_test = np.array(x_test)
  y_test = np.array(y_test)
  x_val = np.array(x_val)
  y_val = np.array(y_val)                   
  del(x)
  return (x_train,y_train,x_test,y_test,x_val,y_val)

path = '/content/drive/My Drive/Dataset/eeg/'
data_path = 'eeg_signals_128_sequential_band_all_with_mean_std.pth'
l_sh = 200       ## diff
r_sh = 50   ## star of time point
diff = l_sh

data1 = torch.load(path + data_path)
dst = data1['dataset']
labels = []
for i in range(len(dst)):
  labels.append(dst[i]['image'])
labels = np.argsort(labels)

x_train,y_train,x_test,y_test,x_val,y_val = load_data()
y_train1,y_test1 = y_train,y_test

y_train = to_categorical(y_train, num_classes=40)
y_val = to_categorical(y_val, num_classes=40)
y_test = to_categorical(y_test, num_classes=40)
x_train = np.transpose(x_train,(0,2,1))#####transpose time First #######
x_val = np.transpose(x_val,(0,2,1))
x_test = np.transpose(x_test,(0,2,1))

"""#Transformer

"""

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        
    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)        
        return self.layernorm2(out1 + ffn_output)


class myCallback(tf.keras.callbacks.Callback):
	def on_epoch_end(self, epoch, logs={}):
		if(logs.get('val_accuracy') == 1.0):
			print("\nReached %2.2f%% accuracy, so stopping training!!" %(100))
			self.model.stop_training = True

num_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer

sequence_input = tf.keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2]))
transformer_block = TransformerBlock(x_train.shape[2], num_heads, ff_dim)
x = transformer_block(sequence_input)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dense(128, activation="relu")(x)
outputs = layers.Dense(40, activation="softmax")(x)
model = tf.keras.Model(inputs=sequence_input, outputs=outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(x_train, y_train, validation_data=(x_test, y_test),
                   epochs=45, batch_size=32,callbacks = myCallback())
print(model.evaluate(x_test, y_test))