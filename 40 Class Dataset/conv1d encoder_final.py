# -*- coding: utf-8 -*-
"""Conv1D_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16NWfg9C6uFajnxfglFtAqzmg2cKS4vA_
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import torch
from tensorflow import keras
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from tensorflow.keras import Input , Model
from tensorflow.keras.layers import Conv1D , MaxPooling1D , Flatten , Dense , UpSampling1D , Reshape ,AveragePooling1D, UpSampling1D, Permute, RepeatVector, Lambda, Multiply, Reshape
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.utils import to_categorical

from torchvision import transforms
import torch.utils.data as data
import numpy as np
#from Dataloader import *
#from model import *
import torch.nn
import torch.optim as optim
from torch.autograd import Variable
import torch

data_dir = '/content/drive/MyDrive/'
batch_size = 128
data_path = 'eeg_signals_128_sequential_band_all_with_mean_std.pth'
split_path = 'block_splits_by_image.pth'
split_no = 3
#loading and normalizing dataset
class eegloader(data.Dataset):
	def __init__(self, data_path, split_path, dtype='train', data_dir='./', split_no=0, dlen=160, stpt=320, nch=128):

		data = torch.load(data_dir + data_path)
		split = torch.load(data_dir + split_path)
    
		self.mean = data['means']
		self.stdev = data['stddevs']
		self.labels = split['splits'][split_no][dtype]

		self.data = []
		for l in self.labels:
			self.data.append(data['dataset'][l])

		assert len(self.data)==len(self.labels)
		self.dlen = dlen
		self.stpt = stpt
		self.nch = nch

	def __len__(self):
		return len(self.data)
	#normalization
	def __getitem__(self, idx):
		nch  = self.nch
		dlen = self.dlen 
		stpt = self.stpt
	
		x = np.zeros((nch,dlen))
		y = self.data[idx]['label']
		s = self.data[idx]['subject'] 
		
		x = torch.from_numpy(x)
		x[:,:min(int(self.data[idx]['eeg'].shape[1]),dlen)] = self.data[idx]['eeg'][:,stpt:stpt+dlen]
		x = x.type(torch.FloatTensor).sub(self.mean.expand(nch,dlen))/ self.stdev.expand(nch,dlen)

		return x, y, s

#preparing data for training
x = eegloader(data_path, split_path, dtype='train', data_dir=data_dir, split_no=split_no)
y = eegloader(data_path, split_path, dtype='val', data_dir=data_dir, split_no=split_no)
z = eegloader(data_path, split_path, dtype='test', data_dir=data_dir, split_no=split_no)

training_x=[]
training_y=[]
testing_x=[]
testing_y=[]
vall_x=[]
vall_y=[]
#train
for i in range(7972):
  training_x.append(tf.make_ndarray(tf.make_tensor_proto(x[i][0])))
  training_y.append(x[i][1])
#test
for i in range(1997):
  testing_x.append(tf.make_ndarray(tf.make_tensor_proto(z[i][0])))
  testing_y.append(z[i][1])
#validation
for i in range(1996):
  vall_x.append(tf.make_ndarray(tf.make_tensor_proto(y[i][0])))
  vall_y.append(y[i][1])


train_x = np.array(training_x)
train_y = np.array(training_y)
test_x = np.array(testing_x)
test_y = np.array(testing_y)
val_x = np.array(vall_x)
val_y = np.array(vall_y)

#sampels of normalized dataset
print(x[1])
print(y[1])
print(z[1])

y_train = to_categorical(train_y, num_classes=40)
y_test = to_categorical(test_y, num_classes=40)

# defining the keras callbacks to be used while training the network


modelcheckpoint = ModelCheckpoint('lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)
earlystopping = EarlyStopping(monitor='val_acc', verbose=1, patience=10)

print(val_x.shape)

#model
input_sig = Input(shape=(128,160)) 
x = Conv1D(128,3, activation='relu', padding='valid',data_format='channels_first')(input_sig)
x_ = MaxPooling1D(2,data_format='channels_first')(x)
x1 = Conv1D(64,3, activation='relu', padding='valid',data_format='channels_first')(x_)
x1_ = MaxPooling1D(2,data_format='channels_first')(x1)
x2 = Conv1D(32,3, activation='relu', padding='valid',data_format='channels_first')(x1_)
x2_ = MaxPooling1D(2,data_format='channels_first')(x2)
flat = Flatten()(x2_)
dense = Dense(128,activation = 'relu')(flat)
encoded = Dense(40,activation = 'softmax')(dense)
autoencoder = Model(input_sig, encoded)
autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

autoencoder.summary()

autoencoder.fit(train_x, y_train, batch_size=128, epochs=75, callbacks=[modelcheckpoint, earlystopping], validation_data=(test_x, y_test))

# model predict
x1 = autoencoder.predict(val_x)
print(np.argmax(x1[1]),
      x1.shape,
      val_y[0:10], 
      sep='\n')

t = 0
f = 0
for i in range(1996):
  tst = np.argmax(x1[i])
  if(tst==val_y[i]):t+=1
  else:f+=1

print(t,f,t/1997)